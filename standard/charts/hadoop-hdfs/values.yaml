## ------------------------------------------------------------------------------
## hdfs-config-k8s:
## ------------------------------------------------------------------------------
hdfs-config-k8s:
  ## Custom hadoop config keys passed to the hdfs configmap as extra keys.
  customHadoopConfig:
     coreSite: {}
      ## Set config key and value pairs, e.g.
      # hadoop.http.authentication.type: kerberos

     hdfsSite: {}
      ## Set config key and value pairs, e.g.
      # dfs.datanode.use.datanode.hostname: "false"

## ------------------------------------------------------------------------------
## hdfs-journalnode-k8s:
## ------------------------------------------------------------------------------
hdfs-journalnode-k8s:

  persistence:
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    ## To choose a suitable persistent volume from available static volumes, selectors
    ## are used.
    # selector:
    #   matchLabels:
    #     volume-type: hdfs-ssd
    accessMode: ReadWriteOnce
    size: 50Gi

  ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}
  hadoopHeapSize: "1024"
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
  metrics:
    port: 8009
    resources: {}
    env:
      JVM_OPTS: " "

## ------------------------------------------------------------------------------
## hdfs-namenode-k8s:
## ------------------------------------------------------------------------------
hdfs-namenode-k8s:
  ## Name of the namenode start script in the config map.
  namenodeStartScript: run.sh
  namenodeFormatScript: format.sh

  ## A namenode start script that can have user specified content.
  ## Can be used to conduct ad-hoc operation as specified by a user.
  ## To use this, also set the namenodeStartScript variable above
  ## to custom-run.sh.
  customRunScript: |
    #!/bin/bash -x
    echo Write your own script content!
    echo This message will disappear in 10 seconds.
    sleep 10

  persistence:
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## To choose a suitable persistent volume from available static volumes, selectors
    ## are used.
    # selector:
    #   matchLabels:
    #     volume-type: hdfs-ssd

    accessMode: ReadWriteOnce

    size: 100Gi

  service:
    enableNodePort: false
    rpc:
      nodePort: 31120
    web:
      nodePort: 31170
  ## Whether or not to use hostNetwork in namenode pods. Disabling this will break
  ## data locality as namenode will see pod virtual IPs and fails to equate them with
  ## cluster node physical IPs associated with data nodes.
  ## We currently disable this only for CI on minikube.
  hostNetworkEnabled: true

  ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}
  hadoopHeapSize: "1024"
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
  metrics:
    port: 8008
    resources: {}
    env:
      JVM_OPTS: " "


## ------------------------------------------------------------------------------
## hdfs-datanode-k8s:
## ------------------------------------------------------------------------------
hdfs-datanode-k8s:
  replicas: 3

  ## Node labels and tolerations for pod assignment
  nodeSelector: {}
  tolerations: []
  affinity: {}
  hadoopHeapSize: "1024"
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
  metrics:
    port: 8017
    resources: {}

    env:
      JVM_OPTS: " "

## ------------------------------------------------------------------------------
## hdfs-krb5-k8s:
## ------------------------------------------------------------------------------
hdfs-krb5-k8s:
  persistence:
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

    ## To choose a suitable persistent volume from available static volumes, selectors
    ## are used.
    # selector:
    #   matchLabels:
    #     volume-type: hdfs-ssd

    accessMode: ReadWriteOnce

    size: 20Gi

  ## We use a 3rd party image built from https://github.com/gcavalcante8808/docker-krb5-server.
  ## TODO: The pod currently prints out the admin account in plain text.
  ## Supply an admin account password using a k8s secret.
  ## TODO: The auto-generated passwords might be weak due to low entropy.
  ## Increase entropy by running rngd or haveged.
  ## TODO: Using latest tag is not desirable. The current image does not have specific tags.
  ## Find a way to fix it.
  image:
    repository: gcavalcante8808/krb5-server

    tag: latest

    pullPolicy: IfNotPresent

  service:
    type: ClusterIP

    port: 88
## ------------------------------------------------------------------------------
## Global values affecting all sub-charts:
## ------------------------------------------------------------------------------
global:
  image:
    repository: registry.mufankong.top/bigdata/hadoop-hdfs
    tag: 2.10.2
  # enable root as proxy user
  enableProxyUserRoot: false

  ## A list of the local disk directories on cluster nodes that will contain the datanode
  ## blocks. These paths will be mounted to the datanode as K8s HostPath volumes.
  ## In a command line, the list should be enclosed in '{' and '}'.
  ## e.g. --set "dataNodeHostPath={/hdfs-data,/hdfs-data1}"
  dataNodeHostPathEnable: false
  dataNodeHostPath: []

  dataNodeHostPathSsd: []

  grafana:
    enabled: true
    dashboardsLabel: grafana_dashboard

#  dataNodeHostPathSsd:
#    - /ssd1/hdfs
#    - /ssd2/hdfs
  namenodeJmxPort: 8003
  datanodeJmxPort: 8004
  journalJmxPort: 8005

  metrics:
    enabled: true
    image: registry.mufankong.top/bigdata/jmx-prometheus-exporter
    imageTag: 0.12.0-openjdk
    pullPolicy: IfNotPresent
    lowercaseOutputName: false
    startDelaySeconds: 30
    livenessProbe:
      failureThreshold: 8
      httpGet:
        path: /metrics
        port: metrics
      initialDelaySeconds: 30
      periodSeconds: 15
      successThreshold: 1
      timeoutSeconds: 60

    readinessProbe:
      failureThreshold: 8
      httpGet:
        path: /metrics
        port: metrics
      initialDelaySeconds: 30
      periodSeconds: 15
      successThreshold: 1
      timeoutSeconds: 60
    # 开启后利用promethus operator上的自动采集机制，能自动采集jmx数据到promethus上
    serviceMonitor:
      enabled: true

  ## Parameters for determining which Unix user and group IDs to use in pods.
  ## Persistent volume permission may need to match these.
  podSecurityContext:
    enabled: false
    runAsUser: 0
    fsGroup: 1000

  ## Whether or not to expect namenodes in the HA setup.
  namenodeHAEnabled: true

  ## The number of zookeeper server to have in the quorum.
  ## This should match zookeeper.replicaCount above. Used only when
  ## namenodeHAEnabled is set.
  zookeeperQuorumSize: 3

  ## Override zookeeper quorum address. Zookeeper is used for determining which namenode
  ## instance is active. Separated by the comma character. Used only when
  ## namenodeHAEnabled is set.
  ##
#  zookeeperQuorumOverride: zookeeper-0.zookeeper-headless.bigdata-dev.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.bigdata-dev.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.bigdata-dev.svc.cluster.local:2181

  ## How many journal nodes to launch as a quorum. Used only when
  ## namenodeHAEnabled is set.
  journalnodeQuorumSize: 3

  ## Whether or not to enable default affinity setting.
  defaultAffinityEnabled: true

  ## Whether or not Kerberos support is enabled.
  kerberosEnabled: false

  ## Effective only if Kerberos is enabled. Override th name of the k8s
  ## ConfigMap containing the kerberos config file.
  ##
  # kerberosConfigMapOverride: kerberos-config

  ## Effective only if Kerberos is enabled. Name of the kerberos config file inside
  ## the config map.
  kerberosConfigFileName: krb5.conf

  ## Effective only if Kerberos is enabled. Override the name of the k8s Secret
  ## containing the kerberos keytab files of per-host HDFS principals.
  ## The secret should have multiple data items. Each data item name
  ## should be formatted as:
  ##    `HOST-NAME.keytab`
  ## where HOST-NAME should match the cluster node
  ## host name that each per-host hdfs principal is associated with.
  ##
  # kerberosKeytabsSecretOverride: hdfs-kerberos-keytabs

  ## Required to be non-empty if Kerberos is enabled. Specify your Kerberos realm name.
  ## This should match the realm name in your Kerberos config file.
  kerberosRealm: MYCOMPANY.COM

  ## Effective only if Kerberos is enabled. Enable protection of datanodes using
  ## the jsvc utility. See the reference doc at
  ## https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SecureMode.html#Secure_DataNode
  jsvcEnabled: true



